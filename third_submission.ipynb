{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "third_submission.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Readme First and Second tasks\n",
        "\n",
        "The first code cell contains the lines of code that enabled the usage of Google Drive files.\n",
        "\n",
        "The following and most important cell of code, contains the commands and the implementation of the AdaBoost algorithm for face detection. \n",
        "\n",
        "1. VideoCapture() is a function with no returned value, which creates the video and specifies its context, such as the type of renderization. For that purpose, in its inside, it was created an async function, which returns a Promise. When the async function returns a value, the Promise will be resolved with that value. The await expression makes it possible to interrupt the execution of the async, and waits for the resolution of the Promise. The purpose of doint this was to apply the Attentional Cascade Algorithm. Then, the face and eyes detection is done using the Cascade Method, which returns rectangles with its detective boundaries.\n",
        "\n",
        "2. byte2image and image2byte, respectively encode and decode an image to a byte-like object, on the ASCII format.\n",
        "\n",
        "3. Implementation of the cascade classifiers is due to its huge improvement of time efficiency. Instead of testing all the features available on a specific window, what is done is that these features are divided in collections, which will be focused on fiding the non-face regions, therefore, minimizing the false positive detection.\n",
        "\n",
        "4. The detect() function makes use of the Cascade Method to identify the rectangles. Its minSize() and maxSize() parameters have the following meaning respectivelly : objects smaller than that are ignored; objects larger than that are ignored. These parameters, together with scaleFactor can improve the time execution, so the algorithm wouldn't lose to much time analysing data that are too large or to small. Since the scaleFactor() specifies how much the image is reduced at each and every image scale, if we have it too small, the time execution will increase.\n",
        "\n",
        "5. Besides, what was done to reduce the computation time corresponds to search for an exception that reduces the area of interest. Like this, the algorithm doesn't waste time on areas not corresponding to the face. "
      ],
      "metadata": {
        "id": "WYusdWe1i0hg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "oul-fmau2Mc-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f5c06c69-8174-4476-a739-ce51ac30c8fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Third and Fourth Tasks\n",
        "\n",
        "1. In this part of the Project, we should be able to improve the detection with the aid of CamShift algorithm. According to its demo version, the mouse is responsible for providing the initial point of the search. \n",
        "\n",
        "2. In the current version of the project, it's necessary to replace that correspondent mouse function by automating it with the AdaBoost algorithm provided by the previous solution, i.e, the face detection. \n",
        "\n",
        "3. The main idea is to make use of the rectangle, i.e the area of interest determined by the face detection code, as the initial points of CamShift, as the track window.  \n",
        "\n",
        "So in the current version, the evaluation of the face detection is limited to the first iteration for which a face is detected, that is, the algorithm will run until it finally finds a face. Once it's done, it stops. Now on, the rectangles extracted will be the track window on which the CamShift algorithm will initiate its search. \n",
        "\n",
        "4. Then, a histogram computation is performed on the area of interest limited to the masked area, the former being the face area on the figure. This feature is currently used for aiming the computation of the histogram to a unique area.\n",
        "\n",
        "5. The back projection is done for measuring hoe well some pixels of a given image are to fit in a given histogram. The parameters of the open cv method corresponds to color and pixels ranges. \n",
        "\n",
        "6. The stopping criteria of the CamShift was fixed to 10 iterations, or to just one single point of difference regarding the former iteration. Therefore, it's the analog of admiting some error epslon. \n",
        "\n",
        "Now, for the fourth task the goal is to erase the face of the image. Ir order to do so, it was necessary to create a condition to extract the probability located in the face area, then setting it to 0. In that way, it was possible to draw the black rectangle. Therefore, the algorithm masks the face in black, enabling the algorithm to identify the hand, since its color map is really similar to the face's. It is convenient to point out that the program may fail if the light conditions are not ideal. Futhermore, it's recommended to try to stand in front of the user camera priorizing your face and whearing long sleeve shirt."
      ],
      "metadata": {
        "id": "_GdCDm1BuEVz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# First to Fourth Task"
      ],
      "metadata": {
        "id": "36LeRdGxPSb9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode #module used to convert binary or text data into ASCII\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import io #module for dealing with several types of I/O\n",
        "import cv2 #Open CV Library\n",
        "import time \n",
        "from datetime import timedelta\n",
        "from google.colab.patches import cv2_imshow\n",
        "import ipywidgets as widgets\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from PIL import Image\n",
        "import csv"
      ],
      "metadata": {
        "id": "pkoUzJgS4Zlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_alt.xml\") #face detection\n",
        "nested = cv2.CascadeClassifier(\"haarcascade_eye.xml\") #eyes detectiond"
      ],
      "metadata": {
        "id": "KXVqkDmQ1ZHK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to decode byte-like object into image\n",
        "def byte2image(byte):\n",
        "  jpeg = b64decode(byte.split(',')[1])\n",
        "  im = Image.open(io.BytesIO(jpeg))\n",
        "  return np.array(im)\n",
        "\n",
        "# Function to do the opposite of byte2image function\n",
        "def image2byte(image):\n",
        "  # Creates an image memory created by exporting the array interface from parameter image\n",
        "  image = Image.fromarray(image)\n",
        "  # Define a buffer to hold the data, which is kept as bytes\n",
        "  buffer = io.BytesIO()\n",
        "  # Save image to the buffer\n",
        "  image.save(buffer, 'jpeg')\n",
        "\n",
        "  buffer.seek(0)  #0 : reference point is in the beggining of the file\n",
        "  x = b64encode(buffer.read()).decode('utf-8')\n",
        "  return x"
      ],
      "metadata": {
        "id": "ODz8ztf9yIg2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function for detecting objects of different sizes based on the input image using the Cascade Method\n",
        "def detect(img, cascade):\n",
        "    # The detected objects are returned as a list of rectangles\n",
        "    # as we update the scale factor by a small factor (0.1~0.2), we can see how time improvement is updated\n",
        "    rects = cascade.detectMultiScale(img, scaleFactor=1.1, minNeighbors=4, minSize=(30, 30),\n",
        "                                     flags=cv2.CASCADE_SCALE_IMAGE) #flags parameter has the same meaning for an old cascade\n",
        "    \n",
        "    if len(rects) == 0:\n",
        "        return []\n",
        "    rects[:,2:] += rects[:,:2]\n",
        "    return rects\n",
        "\n",
        "# Drawn rectangles over the image for recongnition \n",
        "def draw_rects(img, rects, color):\n",
        "    #(x1, y1) are starting coordinates of rectangle; (x2, y2) are ending coordinates\n",
        "    for x1, y1, x2, y2 in rects:\n",
        "        cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n"
      ],
      "metadata": {
        "id": "r8DRj-JxybCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "####################### Face Detection #######################\n",
        "def VideoCapture():\n",
        "  #we create a HTML element with div type\n",
        "  js = Javascript('''\n",
        "    async function create(){\n",
        "      div = document.createElement('div');\n",
        "      document.body.appendChild(div);\n",
        "      //we create the video element and make it playable inline\n",
        "      video = document.createElement('video');\n",
        "      video.setAttribute('playsinline', '');\n",
        "\n",
        "      div.appendChild(video);\n",
        "\n",
        "      //getting access to camera\n",
        "      //video source is facing away from user (i.e to the environment), example : back camera on a smartphone\n",
        "      //stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});  \n",
        "\n",
        "      //video source is facing toward the user (i.e to the environment), example : front camera on a smartphone\n",
        "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"user\"}});\n",
        "\n",
        "      //here we set the stream to be the source of the media associated to video\n",
        "      video.srcObject = stream;\n",
        "\n",
        "      //returns a promise, enabling us to async+await before updating it \n",
        "      await video.play();\n",
        "\n",
        "      canvas =  document.createElement('canvas');\n",
        "      //setting dimensions of canvas element\n",
        "      canvas.width = video.videoWidth; \n",
        "      canvas.height = video.videoHeight;\n",
        "      //create a bidimensional renderization object and draw the video into the context\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "\n",
        "      div_out = document.createElement('div');\n",
        "      document.body.appendChild(div_out);\n",
        "      img = document.createElement('img');\n",
        "      div_out.appendChild(img);\n",
        "    }\n",
        "\n",
        "    async function capture(){\n",
        "        return await new Promise(function(resolve, reject){\n",
        "            pendingResolve = resolve;\n",
        "            canvas.getContext('2d').drawImage(video, 0, 0); \n",
        "            result = canvas.toDataURL('image/jpeg', 0.8);\n",
        "            pendingResolve(result);\n",
        "        })\n",
        "    }\n",
        "\n",
        "    function showimg(imgb64){\n",
        "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
        "    }\n",
        "\n",
        "  ''')\n",
        "  display(js)\n",
        "\n",
        "# It allows access to computer camera using JavaScript\n",
        "VideoCapture()\n",
        "eval_js('create()')\n",
        "\n",
        "cascade = cv2.CascadeClassifier(\"haarcascade_frontalface_alt.xml\") #face detection\n",
        "nested = cv2.CascadeClassifier(\"haarcascade_eye.xml\") #eyes detection\n",
        "\n",
        "####################### DETECT TRACK WINDOW #######################\n",
        "\n",
        "track_window = [] # Empty list\n",
        "                  # Variable to store the coordinates of rectangles retrieved from Face Detection\n",
        "\n",
        "while len(track_window) == 0:\n",
        "  byte = eval_js('capture()')\n",
        "  im = byte2image(byte)\n",
        "  # Convert the original colorspace of im to COLOR_BGR2GRAY\n",
        "  gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n",
        "  track_window = detect(gray, cascade)\n",
        "  vis = im.copy() # L. 119\n",
        "  draw_rects(vis, track_window, (0, 255, 0))\n",
        "\n",
        "x0, y0, x1, y1 = track_window[0]\n",
        "hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n",
        "mask = cv2.inRange(hsv, np.array((0., 60., 32.)), np.array((180., 255., 255.))) # Area of the face\n",
        "hsv_roi = hsv[y0:y1, x0:x1]\n",
        "mask_roi = mask[y0:y1, x0:x1]\n",
        "\n",
        "# Computing the histogram of hsv_roi for the masked pixels, [0] corresponds to computation in gray-scale\n",
        "hist = cv2.calcHist( [hsv_roi], [0], mask_roi, [16], [0, 180] )\n",
        "cv2.normalize(hist, hist, 0, 255, cv2.NORM_MINMAX)\n",
        "\n",
        "# Stop criteria\n",
        "term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n",
        "\n",
        "while True: \n",
        "  byte = eval_js('capture()')\n",
        "  im = byte2image(byte)\n",
        "  vis = im.copy()\n",
        "\n",
        "  gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n",
        "  mask = cv2.inRange(hsv, np.array((0., 60., 32.)), np.array((180., 255., 255.))) # Area of the face\n",
        "  hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n",
        "  prob = cv2.calcBackProject([hsv], [0], hist, [0, 180], 1)\n",
        "  prob &= mask\n",
        "\n",
        "  ####################### Performing Camshift Over Track Window #######################\n",
        "  rect, track_window = cv2.CamShift(prob, track_window, term_crit)\n",
        "  x, y, w, h = track_window\n",
        "  img2 = cv2.rectangle(im, (x, y), (x+w, y+h), 255, 2)\n",
        "\n",
        "  # Erase the face area \n",
        "  prob[y:y+h,x:x+w]=0\n",
        "  \n",
        "  # Add another camshift here to detect hand\n",
        "  byte = eval_js('capture()')\n",
        "  im = byte2image(byte)\n",
        "  im_w = im.shape[1] # Image width \n",
        "  im_h = im.shape[0] # Image heigth \n",
        "\n",
        "  # Here, Camshift is executed with prob on face region put to 0\n",
        "  track_window_hand = (0, 0, im_w, im_h)\n",
        "  rect_h, track_window_hand = cv2.CamShift(prob, track_window_hand, term_crit)\n",
        "  xH, yH, wH, hH = track_window_hand\n",
        " \n",
        "  img2 = cv2.rectangle(prob, (xH,yH), (xH+wH, yH+hH), 255, 2)\n",
        "\n",
        "  eval_js('showimg(\"{}\")'.format(image2byte(img2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_gheX__stmT5",
        "outputId": "ef0eefc8-49ae-43fd-d156-070460897767"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function create(){\n",
              "      div = document.createElement('div');\n",
              "      document.body.appendChild(div);\n",
              "      //we create the video element and make it playable inline\n",
              "      video = document.createElement('video');\n",
              "      video.setAttribute('playsinline', '');\n",
              "\n",
              "      div.appendChild(video);\n",
              "\n",
              "      //getting access to camera\n",
              "      //video source is facing away from user (i.e to the environment), example : back camera on a smartphone\n",
              "      //stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});  \n",
              "\n",
              "      //video source is facing toward the user (i.e to the environment), example : front camera on a smartphone\n",
              "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"user\"}});\n",
              "\n",
              "      //here we set the stream to be the source of the media associated to video\n",
              "      video.srcObject = stream;\n",
              "\n",
              "      //returns a promise, enabling us to async+await before updating it \n",
              "      await video.play();\n",
              "\n",
              "      canvas =  document.createElement('canvas');\n",
              "      //setting dimensions of canvas element\n",
              "      canvas.width = video.videoWidth; \n",
              "      canvas.height = video.videoHeight;\n",
              "      //create a bidimensional renderization object and draw the video into the context\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "\n",
              "      div_out = document.createElement('div');\n",
              "      document.body.appendChild(div_out);\n",
              "      img = document.createElement('img');\n",
              "      div_out.appendChild(img);\n",
              "    }\n",
              "\n",
              "    async function capture(){\n",
              "        return await new Promise(function(resolve, reject){\n",
              "            pendingResolve = resolve;\n",
              "            canvas.getContext('2d').drawImage(video, 0, 0); \n",
              "            result = canvas.toDataURL('image/jpeg', 0.8);\n",
              "            pendingResolve(result);\n",
              "        })\n",
              "    }\n",
              "\n",
              "    function showimg(imgb64){\n",
              "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
              "    }\n",
              "\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-9a216b65b7d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m   \u001b[0;31m# Add another camshift here to detect hand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m   \u001b[0mbyte\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'capture()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m   \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbyte2image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m   \u001b[0mim_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# Image width\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we want to create two data sets, respectively dataset16 and dataset224 for 16x16 and 224x224 images. Those images correspond to the hand, selected by the CamShift algorithm performed over the area of interest. "
      ],
      "metadata": {
        "id": "0f5juFLZTO22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fifth task:"
      ],
      "metadata": {
        "id": "T2HFIyEhOROQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset16 = \"/content/gdrive/MyDrive/Computer Vision/dataset16/\"\n",
        "dataset224 = \"/content/gdrive/MyDrive/Computer Vision/dataset224/\""
      ],
      "metadata": {
        "id": "s70kF-pNOU1g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VideoCapture()\n",
        "eval_js('create()')\n",
        "\n",
        "byte = eval_js('capture()') \n",
        "im = byte2image(byte)\n",
        "im_w = im.shape[1] # Image width \n",
        "im_h = im.shape[0] # Image heigth\n",
        "tracking_window_hand = (0, 0, im_w, im_h)\n",
        "\n",
        "################################### Perform the Face Detection ###################################\n",
        "\n",
        "track_window = [] # Variable to store rectangle coordinates \n",
        "\n",
        "while len(track_window)==0:\n",
        "  byte = eval_js('capture()')\n",
        "  im = byte2image(byte)\n",
        "  gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n",
        "  ## Face detection\n",
        "  track_window = detect(gray, cascade)\n",
        "  vis = im.copy()\n",
        "  draw_rects(vis, track_window, (0, 255, 0))\n",
        "\n",
        "################################### Perform CamShift ###################################\n",
        "\n",
        "x0, y0, x1, y1 = track_window[0] # get the coordinates of rectangle, [0] since we have only one face \n",
        "\n",
        "# ROI for CamShift\n",
        "hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n",
        "mask = cv2.inRange(hsv, np.array((0., 60., 32.)), np.array((180., 255., 255.)))\n",
        "hsv_roi = hsv[y0:y0+y1, x0:x0+x1]\n",
        "mask_roi = mask[y0:y0+y1, x0:x0+x1]\n",
        "hist = cv2.calcHist( [hsv_roi], [0], mask_roi, [16], [0, 180] )\n",
        "cv2.normalize(hist, hist, 0, 255, cv2.NORM_MINMAX)\n",
        "\n",
        "# Termination criteria, either 10 iterations or move by at least 1 pt\n",
        "term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n",
        "print(term_crit)\n",
        "\n",
        "# n = number of images to store \n",
        "n = 1\n",
        "\n",
        "# Here, still, we perform the CamShift\n",
        "for k in range(n):\n",
        "  byte = eval_js('capture()')\n",
        "  im = byte2image(byte)\n",
        "  vis = im.copy()\n",
        "  gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n",
        "  mask = cv2.inRange(hsv, np.array((0., 60., 32.)), np.array((180., 255., 255.)))\n",
        "  hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n",
        "  prob = cv2.calcBackProject([hsv],[0],hist,[0,180],1)\n",
        "  prob &= mask\n",
        "  # Now performing CamShift to update the coordinates\n",
        "  ret, track_window = cv2.CamShift(prob, track_window, term_crit)\n",
        "  # Draw it on image\n",
        "  x, y, w, h = track_window\n",
        "  img2 = cv2.rectangle(im, (x,y), (x+w,y+h), 255,2)\n",
        " \n",
        "  # Erase the face area from prob\n",
        "  prob[0:im_h,x-20:x+w+20] = 0   # Here we put a margin = 20 \n",
        "\n",
        "  # Hand detection\n",
        "  res_h, tracking_window_hand = cv2.CamShift(prob, tracking_window_hand, term_crit)\n",
        "  xh,yh,wh,hh = tracking_window_hand\n",
        "  img2 = cv2.rectangle(im, (xh,yh), (xh+wh,yh+hh), 255,2)\n",
        "\n",
        "  hand_crop = img2[0:im_h,0:x-20]\n",
        "  hand_crop_prob = prob[0:im_h,0:x-20]\n",
        "\n",
        "  # Store 16x16 and 224x224 images to Drive \n",
        "  hand16 = cv2.resize(hand_crop_prob,(16,16))\n",
        "  hand224 = cv2.resize(hand_crop_prob,(224,224))\n",
        "  \n",
        "  path16 = dataset16+'test16.jpg'\n",
        "  path224 = dataset224+'test224.jpg'\n",
        "  \n",
        "  cv2.imwrite(path16, hand16)\n",
        "  cv2.imwrite(path224, hand224)\n",
        "  \n",
        "  eval_js('showimg(\"{}\")'.format(image2byte(prob)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4od68U6OOg6L",
        "outputId": "c5faddcc-fccb-4a84-a70d-73c329a42b7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function create(){\n",
              "      div = document.createElement('div');\n",
              "      document.body.appendChild(div);\n",
              "      //we create the video element and make it playable inline\n",
              "      video = document.createElement('video');\n",
              "      video.setAttribute('playsinline', '');\n",
              "\n",
              "      div.appendChild(video);\n",
              "\n",
              "      //getting access to camera\n",
              "      //video source is facing away from user (i.e to the environment), example : back camera on a smartphone\n",
              "      //stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});  \n",
              "\n",
              "      //video source is facing toward the user (i.e to the environment), example : front camera on a smartphone\n",
              "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"user\"}});\n",
              "\n",
              "      //here we set the stream to be the source of the media associated to video\n",
              "      video.srcObject = stream;\n",
              "\n",
              "      //returns a promise, enabling us to async+await before updating it \n",
              "      await video.play();\n",
              "\n",
              "      canvas =  document.createElement('canvas');\n",
              "      //setting dimensions of canvas element\n",
              "      canvas.width = video.videoWidth; \n",
              "      canvas.height = video.videoHeight;\n",
              "      //create a bidimensional renderization object and draw the video into the context\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "\n",
              "      div_out = document.createElement('div');\n",
              "      document.body.appendChild(div_out);\n",
              "      img = document.createElement('img');\n",
              "      div_out.appendChild(img);\n",
              "    }\n",
              "\n",
              "    async function capture(){\n",
              "        return await new Promise(function(resolve, reject){\n",
              "            pendingResolve = resolve;\n",
              "            canvas.getContext('2d').drawImage(video, 0, 0); \n",
              "            result = canvas.toDataURL('image/jpeg', 0.8);\n",
              "            pendingResolve(result);\n",
              "        })\n",
              "    }\n",
              "\n",
              "    function showimg(imgb64){\n",
              "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
              "    }\n",
              "\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 10, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sixth Task"
      ],
      "metadata": {
        "id": "BgKfSx9dO2EE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Input string to get the letter the user wants to store\n",
        "letter = str(input('\\nInsert the letter (uppercase): '))\n",
        "# Input integer to inform how many samples \n",
        "max = int(input('\\nInsert the maximum number of pictures you want to stock: ')) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Frbr50o0SF6u",
        "outputId": "7f93e4e7-0e9f-4174-87f3-b4a992493f36"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Insert the letter (uppercase): A\n",
            "\n",
            "Insert the maximum number of pictures you want to stock: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "VideoCapture()\n",
        "eval_js('create()')\n",
        "\n",
        "byte = eval_js('capture()') \n",
        "im = byte2image(byte)\n",
        "im_w = im.shape[1] # Image width \n",
        "im_h = im.shape[0] # Image heigth\n",
        "tracking_window_hand = (0, 0, im_w, im_h)\n",
        "\n",
        "###################################  Loop to detect the face once ################################### \n",
        "\n",
        "track_window = [] # New variable where we will store the rectangle's coordinates\n",
        "\n",
        "while len(track_window)==0:\n",
        "  byte = eval_js('capture()')\n",
        "  im = byte2image(byte)\n",
        "  gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n",
        "  ## Face detection\n",
        "  track_window = detect(gray, cascade)\n",
        "  vis = im.copy()\n",
        "  draw_rects(vis, track_window, (0, 255, 0))\n",
        "\n",
        "################################### CamShift ###################################\n",
        "\n",
        "x0, y0, x1, y1 = track_window[0] # Get coordinates of rectangle corresponding to face\n",
        "\n",
        "# Initiate the ROI for CamShift\n",
        "hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n",
        "mask = cv2.inRange(hsv, np.array((0., 60., 32.)), np.array((180., 255., 255.)))\n",
        "hsv_roi = hsv[y0:y0+y1, x0:x0+x1]\n",
        "mask_roi = mask[y0:y0+y1, x0:x0+x1]\n",
        "hist = cv2.calcHist( [hsv_roi], [0], mask_roi, [16], [0, 180] )\n",
        "cv2.normalize(hist, hist, 0, 255, cv2.NORM_MINMAX)\n",
        "\n",
        "# Setup the termination criteria, either 10 iteration or move by atleast 1 pt\n",
        "term_crit = ( cv2.TERM_CRITERIA_EPS | cv2.TERM_CRITERIA_COUNT, 10, 1 )\n",
        "\n",
        "#CamShift\n",
        "for k in range(max):\n",
        "  byte = eval_js('capture()')\n",
        "  im = byte2image(byte)\n",
        "  vis = im.copy()\n",
        "  gray = cv2.cvtColor(im, cv2.COLOR_RGB2GRAY)\n",
        "  mask = cv2.inRange(hsv, np.array((0., 60., 32.)), np.array((180., 255., 255.)))\n",
        "  hsv = cv2.cvtColor(im, cv2.COLOR_BGR2HSV)\n",
        "  prob = cv2.calcBackProject([hsv],[0],hist,[0,180],1)\n",
        "  prob &= mask\n",
        "  # CamShift update location\n",
        "  ret, track_window = cv2.CamShift(prob, track_window, term_crit)\n",
        "  # Draw it on image\n",
        "  x, y, w, h = track_window\n",
        "  img2 = cv2.rectangle(im, (x,y), (x+w,y+h), 255,2)\n",
        " \n",
        "  # Erase the face \n",
        "  prob[0:im_h,x-20:x+w+20]=0   \n",
        "\n",
        "  # CamShift to track the hand\n",
        "  res_h, tracking_window_hand = cv2.CamShift(prob, tracking_window_hand, term_crit)\n",
        "  xh,yh,wh,hh = tracking_window_hand\n",
        "  img2 = cv2.rectangle(im, (xh,yh), (xh+wh,yh+hh), 255,2)\n",
        " \n",
        "  hand_crop = img2[0:im_h,0:x-20]\n",
        "  hand_crop_prob = prob[0:im_h,0:x-20]\n",
        "\n",
        "  # save 16x16 and 224x224 images to Drive \n",
        "  hand16 = cv2.resize(hand_crop_prob, (16,16))\n",
        "  hand224 = cv2.resize(hand_crop_prob, (224,224))\n",
        "  \n",
        "  path16 = dataset16+letter.upper()+'_'+str(k)+'_16.jpg'\n",
        "  path224 = dataset224+letter.upper()+'_'+str(k)+'_224.jpg'\n",
        "  \n",
        "  cv2.imwrite(path16, hand16)\n",
        "  cv2.imwrite(path224, hand224)\n",
        "  \n",
        "  eval_js('showimg(\"{}\")'.format(image2byte(prob)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 985
        },
        "id": "Ut1tDB6rOlBU",
        "outputId": "6fef3a98-3199-4e1c-b8bd-8e224a73ad1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function create(){\n",
              "      div = document.createElement('div');\n",
              "      document.body.appendChild(div);\n",
              "      //we create the video element and make it playable inline\n",
              "      video = document.createElement('video');\n",
              "      video.setAttribute('playsinline', '');\n",
              "\n",
              "      div.appendChild(video);\n",
              "\n",
              "      //getting access to camera\n",
              "      //video source is facing away from user (i.e to the environment), example : back camera on a smartphone\n",
              "      //stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"environment\"}});  \n",
              "\n",
              "      //video source is facing toward the user (i.e to the environment), example : front camera on a smartphone\n",
              "      stream = await navigator.mediaDevices.getUserMedia({video: {facingMode: \"user\"}});\n",
              "\n",
              "      //here we set the stream to be the source of the media associated to video\n",
              "      video.srcObject = stream;\n",
              "\n",
              "      //returns a promise, enabling us to async+await before updating it \n",
              "      await video.play();\n",
              "\n",
              "      canvas =  document.createElement('canvas');\n",
              "      //setting dimensions of canvas element\n",
              "      canvas.width = video.videoWidth; \n",
              "      canvas.height = video.videoHeight;\n",
              "      //create a bidimensional renderization object and draw the video into the context\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "\n",
              "      div_out = document.createElement('div');\n",
              "      document.body.appendChild(div_out);\n",
              "      img = document.createElement('img');\n",
              "      div_out.appendChild(img);\n",
              "    }\n",
              "\n",
              "    async function capture(){\n",
              "        return await new Promise(function(resolve, reject){\n",
              "            pendingResolve = resolve;\n",
              "            canvas.getContext('2d').drawImage(video, 0, 0); \n",
              "            result = canvas.toDataURL('image/jpeg', 0.8);\n",
              "            pendingResolve(result);\n",
              "        })\n",
              "    }\n",
              "\n",
              "    function showimg(imgb64){\n",
              "        img.src = \"data:image/jpg;base64,\" + imgb64;\n",
              "    }\n",
              "\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for dirname, _, filenames in os.walk(dataset16):\n",
        "  for image in filenames:\n",
        "    im = Image.open(image)\n",
        "    im_array = np.asarray(im).ravel().tolist()\n",
        "    im_array.insert(0,image[0])\n",
        "    with open('dataset.txt', 'a',newline='') as outfile:\n",
        "      writer = csv.writer(outfile)\n",
        "      writer.writerow(im_array)"
      ],
      "metadata": {
        "id": "XB1GRe8cO7CZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lines = open('dataset.txt').readlines()\n",
        "random.shuffle(lines)\n",
        "open('dataset.txt', 'w').writelines(lines)\n",
        "print('\\nDone.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "NDkYL9P2PHKL",
        "outputId": "6a6fed3d-7976-4e6e-f5d0-8a69e015b1f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-10783d0a2d0f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dataset.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwritelines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nDone.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset.txt'"
          ]
        }
      ]
    }
  ]
}